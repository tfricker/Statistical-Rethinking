---
output: html_document
editor_options: 
  chunk_output_type: console
---
### Statistcial Rethinking

Code associated with Lecture 3 of Richard's Winter course.

Begin by loading packages
```{r}
#install.packages(c("devtools","mvtnorm","loo","coda"),dependencies=TRUE)
library(devtools)
library(brms)
library(ggplot2)
library(dplyr)
library(tidyr)
#install_github("rmcelreath/rethinking")
```

## Lecture 3

Load the data used in the chapter.
```{r}
library(rethinking)
data(Howell1)
d <- Howell1
```

Look at the structure and a minimalist summary of the data.
```{r}
str(d)
precis(d)
```

Filter the data frame to indiviudals of age 18 or greater.
```{r}
d2 <- d %>%
  filter(age >= 18)
```

The Model
---------
Whatever the prior, it’s a very good idea to plot your priors, so you have a sense of the
assumption they build into the model. In this case:
```{r}
df = data.frame(x = seq(from = 100, to = 250, by = .1))

ggplot(data = df, aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  ylab("density") +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

```{r}
df = data.frame(x = seq(from = -10, to = 60, by = .1))

ggplot(data = df, aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

An essential part to modeling is the prior predicitive check. Once you’ve chosen priors for $h$, \mu, and \sigma, these imply a joint prior distribution of individual heights. By simulating from this distribution, you can see what your choices imply about observable height. This helps you diagnose bad choices. Also, remember every posterior is also
potentially a prior for a subsequent analysis, so you can process priors just like posteriors.
```{r}
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
```

Prior predictive simulation is very useful for assigning sensible priors, because it can
be quite hard to anticipate how priors influence the observable variables. As an example,
consider a much flatter and less informative prior for \mu, like \mu ~ Normal(178, 100). Priors with such large standard deviations are quite common in Bayesian models, but the are hardly ever sensible. Let’s use simulation again to see the implied heights:
```{r}
n <- 1000

df = data.frame(sample_mu = rnorm(n, mean = 178, sd = 20),
                sample_sigma = runif(n, min = 0, max = 50)) %>%
  mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma))

 ggplot(data = df, aes(x = x)) +
  geom_density(fill = "black", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

Grid approximation
------------------

```{r}
n <- 200

d_grid <-data.frame(mu = seq(from = 140, to = 160, length.out = n),
                    sigma = seq(from = 4, to = 9, length.out = n)) %>% 
  # we'll accomplish with `tidyr::expand()` what McElreath did with base R `expand.grid()`
  expand(mu, sigma)

head(d_grid)

d_grid$LL <- sapply( 1:nrow(d_grid) , function(i) sum( dnorm(
d2$height ,
mean=d_grid$mu[i] ,
sd=d_grid$sigma[i] ,
log=TRUE ) ) )

d_grid <- d_grid %>%
  mutate(log_likelihood = LL,
       prior_mu = dnorm(mu, mean = 178, sd  = 20, log = T),
       prior_sigma = dunif(sigma, min  = 0,   max = 50, log = T)) %>% 
  mutate(product = log_likelihood + prior_mu + prior_sigma) %>% 
  mutate(probability = exp(product - max(product)))

head(d_grid)
```

Inspect the posterior distribution. First with a simple contour plot and then with a simple heat map.
```{r}
d_grid %>% 
  ggplot(aes(x = mu, y = sigma, z = probability)) + 
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid$mu),
                  ylim = range(d_grid$sigma)) +
  theme(panel.grid = element_blank())
```

```{r}
d_grid %>% 
  ggplot(aes(x = mu, y = sigma)) + 
  geom_raster(aes(fill = probability),
              interpolate = T) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

Sample from the posterior.
--------------------------
```{r}
d_grid_samples <- 
  d_grid %>% 
  sample_n(size = 1e4, replace = T, weight = probability)
```

You end up with 10,000 samples, with replacement, from the posterior for the height data.

Take a look at these samples:
```{r}
d_grid_samples %>% 
  ggplot(aes(x = mu, y = sigma)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

To characterize the shapes of the marginal posterior densities of \mu and \sigma, all we need to do is:
```{r}
d_grid_samples %>% 
  select(mu, sigma) %>% 
  gather() %>%
  ggplot(aes(x = value)) + 
  geom_density(fill = "grey33", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = "free")
```

```{r}
library(tidybayes)

d_grid_samples %>% 
  select(mu, sigma) %>% 
  gather() %>% 
  group_by(key) %>% 
  mode_hdi(value)
```


Finding the posterior with brms---HMC
---------------------------------------------------------
```{r}
b4.1_half_cauchy <- 
  brm(data = d2, family = gaussian,
      formula = height ~ 1,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(cauchy(0, 1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4)
```

Look at the posterior distribution
```{r}
plot(b4.1_half_cauchy)
```

Check the summary
```{r}
b4.1_half_cauchy$fit
```

Sampling from a brm() fit
```{r}
post <- posterior_samples(b4.1_half_cauchy)

head(post)

posterior_summary(b4.1_half_cauchy)
```

Adding a predictor to the model
```{r}
b4.3 <- 
  brm(data = d2, family = gaussian,
      formula = height ~ 1 + weight,
      prior = c(prior(normal(156, 100), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1), class = sigma)),
      iter = 41000, warmup = 40000, chains = 4, cores = 4)

plot(b4.3)
```

Interpreting the model fit
```{r}
posterior_summary(b4.3)[1:3, ]
```

Look at correlations in model
```{r}
posterior_samples(b4.3) %>%
  select(-lp__) %>%
  cor() %>%
  round(digits = 2)
```

With centering, we can reduce the correlations among the parameters.

```{r}
d2 <- d2 %>%
  mutate(weight_c = weight - mean(weight))

```
Fit the weight_c model, b4.4.

```{r}
b4.4 <- 
  brm(data = d2, family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 100), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1), class = sigma)),
      iter = 46000, warmup = 45000, chains = 4, cores = 4,
      seed = 4)

plot(b4.4)

posterior_summary(b4.4)[1:3, ]
```

Check correlation information
```{r}
posterior_samples(b4.4) %>%
  select(-lp__) %>%
  cor() %>%
  round(digits = 2)

pairs(b4.4)
```

Much better now.

Plot the posterior inference against the data
```{r}
d2 %>%
  ggplot(aes(x = weight, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1], 
              slope     = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_bw() +
  theme(panel.grid = element_blank())
```


